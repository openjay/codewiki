{
  "providers": [
    {
      "provider": "lm_studio",
      "base_url": "http://localhost:1234",
      "models": ["llama-3.2-3b-instruct"],
      "max_tokens": 8192,
      "timeout_seconds": 30,
      "max_retries": 3,
      "cost_per_1k_tokens": 0.0,
      "priority": 1,
      "enabled": true,
      "notes": "PRIMARY (optimized): LM Studio is 2-3x faster than Ollama, lower CPU usage. Use for daily runs."
    },
    {
      "provider": "ollama",
      "base_url": "http://localhost:11434",
      "models": ["qwen3:8b"],
      "max_tokens": 4096,
      "timeout_seconds": 30,
      "max_retries": 3,
      "cost_per_1k_tokens": 0.0,
      "priority": 2,
      "enabled": true,
      "notes": "BACKUP: Ollama qwen3:8b is more accurate but slower. Use for weekly deep scans."
    },
    {
      "provider": "openai",
      "api_key": "${OPENAI_API_KEY}",
      "models": ["gpt-4", "gpt-4-turbo", "gpt-3.5-turbo"],
      "max_tokens": 4096,
      "temperature": 0.7,
      "timeout_seconds": 60,
      "max_retries": 3,
      "cost_per_1k_tokens": 0.03,
      "priority": 3,
      "enabled": false
    },
    {
      "provider": "anthropic",
      "api_key": "${ANTHROPIC_API_KEY}",
      "models": ["claude-3-opus", "claude-3-sonnet", "claude-3-haiku"],
      "max_tokens": 4096,
      "temperature": 0.7,
      "timeout_seconds": 60,
      "max_retries": 3,
      "cost_per_1k_tokens": 0.015,
      "priority": 4,
      "enabled": false
    },
    {
      "provider": "groq",
      "api_key": "${GROQ_API_KEY}",
      "models": ["mixtral-8x7b", "llama2-70b"],
      "max_tokens": 4096,
      "temperature": 0.7,
      "timeout_seconds": 30,
      "max_retries": 3,
      "cost_per_1k_tokens": 0.0002,
      "priority": 5,
      "enabled": false
    }
  ],
  "default_routing_strategy": "cost_optimized",
  "health_check_interval_seconds": 60,
  "max_consecutive_failures": 3,
  "failure_recovery_time_seconds": 300,
  "_optimization_notes": {
    "version": "1.0",
    "date": "2025-12-19",
    "changes": [
      "Swapped LM Studio to priority 1 (faster inference)",
      "Ollama moved to priority 2 (backup for accuracy)",
      "Updated model name for LM Studio to match actual model",
      "Added optimization notes for future reference"
    ],
    "expected_improvements": {
      "runtime": "-50% to -66% (2-3x speedup)",
      "cpu_usage": "-30% to -40% per request",
      "fan_noise": "-50% to -60% reduction",
      "accuracy": "~5-10% lower (acceptable for daily runs)"
    }
  }
}

